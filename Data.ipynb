{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557a1c37-bca7-4e67-91e6-a8b49be059ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import csv\n",
    "import os\n",
    "import hashlib\n",
    "import logging\n",
    "from subprocess import run as srun\n",
    "from urllib.parse import urlparse\n",
    "from itertools import product\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f454f32-3d44-4a50-b10e-68adf554ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d225ba-2f74-472f-8842-13ff003c2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(uri, ofile, md5):\n",
    "    srun(['curl', '-s', '-o', ofile, uri], capture_output=True, check=True)\n",
    "    md5dld = str(hashlib.md5(open(ofile, 'rb').read()).hexdigest())\n",
    "    if md5 != md5dld:\n",
    "        logging.warning(\"%s != %s\", md5, md5dld)\n",
    "        logging.info(\"uri %s (%s == %s) : %s\", uri, md5, md5dld, ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e5a52-afd8-4345-be3f-aed4b162d27e",
   "metadata": {},
   "source": [
    "**FIRST CODE TRYING TO DOWNLOAD DATA (WHOLE RASTER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123a5417-b4c9-4062-a787-d2a39a96841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years=[1985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452cc647-e414-46fa-80c2-a1aede9e2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models=['EC-Earth3_','TaiESM1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27caa0e7-b25c-4517-8569-4a37b4524dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios=['historical','ssp126','ssp370']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e46ac2-1cd4-4261-b653-6175186731eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables=['pr','sfcWind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95921142-eadb-4394-b511-7f1820fd76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_1=list(range(2025, 2046))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210a92da-2872-4bc9-a74e-a7d7906ab3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_2=list(range(2045, 2066))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67e7922-93e2-467f-9413-607da258b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gddp-cmip6-thredds-fileserver.csv') as index:\n",
    "#     fobjects = csv.reader(index)\n",
    "#     next(fobjects)\n",
    "#     for objs in fobjects:\n",
    "#         md5, uri = [o.strip() for o in objs]\n",
    "#         prsout = urlparse(uri)\n",
    "#         ofile = os.path.split(prsout.path)[1]\n",
    "\n",
    "#         if (any(str(year) in ofile for year in years) and\n",
    "#             any(model in ofile for model in models) and\n",
    "#             any(variable in ofile for variable in variables) and\n",
    "#             any(scenario in ofile for scenario in scenarios)):\n",
    "\n",
    "#             print(f\"Downloading: {ofile}\")  # Debug print\n",
    "#             download(uri, ofile, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbcf48-04ff-4cc8-ac17-30d3498ccdbc",
   "metadata": {},
   "source": [
    "**DOWNLOAD RASTER ONLY FOR URUGUAY REGION AND COMPUTES MONTHLY ACCUMULATED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e446433-c91f-48ed-83b9-fc6c968c270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_download(csv_path, years, models, variables, scenarios, shape):\n",
    "    with open(csv_path) as index:\n",
    "        fobjects = csv.reader(index)\n",
    "        next(fobjects)  # Skip header\n",
    "\n",
    "        # Filter and download each file that matches the criteria\n",
    "        for objs in fobjects:\n",
    "            md5, uri = [o.strip() for o in objs]\n",
    "\n",
    "            # Extract the filename from the URL path\n",
    "            prsout = urlparse(uri)\n",
    "            ofile = os.path.split(prsout.path)[1]\n",
    "    \n",
    "            # Download the file only if it matches the filtering criteria\n",
    "            if any(str(year) in ofile for year in years) and \\\n",
    "               any(model in ofile for model in models) and \\\n",
    "               any(variable in ofile for variable in variables) and \\\n",
    "               any(scenario in ofile for scenario in scenarios):\n",
    "        \n",
    "                print(f\"Downloading: {ofile}\")  # Debug print\n",
    "        \n",
    "                # Download the full data file\n",
    "                download(uri, ofile, md5)\n",
    "\n",
    "                filename = os.path.splitext(os.path.basename(uri))[0]\n",
    "        \n",
    "                # Open the dataset after downloading\n",
    "                data = os.path.join(os.getcwd(), ofile)  # Update with correct path\n",
    "                ds_GDDP6 = xr.open_dataset(data)\n",
    "        \n",
    "                # Set spatial dims and CRS\n",
    "                ds_GDDP6 = ds_GDDP6.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "                ds_GDDP6.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "                ds_GDDP6.coords['lon'] = (ds_GDDP6.coords['lon'] + 180) % 360 - 180\n",
    "                ds_GDDP6 = ds_GDDP6.sortby(ds_GDDP6.lon)\n",
    "        \n",
    "                # Clip the data to the Uruguay region\n",
    "                ds_GDDP6 = ds_GDDP6.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "                ds_masked = ds_GDDP6.rio.clip(shape.geometry, shape.crs, drop=True)\n",
    "\n",
    "                if 'pr' in variables:\n",
    "                # if variables=='pr':\n",
    "                    # Convert and round the precipitation data (daily precipitation in mm/day)\n",
    "                    secs_in_a_day = 24 * 60 * 60\n",
    "                    ds_masked['pr_mmd'] = (ds_masked['pr'] * secs_in_a_day).round(2).astype('float32')\n",
    "        \n",
    "                # Save the filtered data to a NetCDF file\n",
    "                ds_masked.to_netcdf(f\"filtered_data/{filename}_masked.nc\")\n",
    "        \n",
    "                # Delete the original unfiltered file to save space\n",
    "                os.remove(data)\n",
    "                print(f\"Deleted original unfiltered file: {ofile}\")\n",
    "        \n",
    "                # Explicitly close datasets to free memory\n",
    "                ds_GDDP6.close()\n",
    "                ds_masked.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169e13ad-3fd3-4f1f-bc32-33a00b7c497c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pyogrio/geopandas.py:265: UserWarning: More than one layer found in 'ury_adm_2020_shp.zip': 'ury_admbnda_adm0_2020' (default), 'ury_admbnda_adm1_2020', 'ury_admbnda_adm2_2020', 'ury_admbndl_ALL_2020'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n"
     ]
    }
   ],
   "source": [
    "# Define the shapefile for Uruguay area (using GeoPandas)\n",
    "shape = gpd.read_file(\"ury_adm_2020_shp.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae367ea-e6e6-493f-9794-60b6dc9b33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the filtering parameters\n",
    "years = list(range(2025, 2044)) #list(range(2055, 2065)) # #list(range(1985, 2015))\n",
    "models= ['TaiESM1'] #['TaiESM1'] #['EC-Earth3_'] \n",
    "variables = ['pr'] #['sfcWind'] #[\"pr\"] #['tas_']\n",
    "scenarios = ['ssp370'] #[\"historical\"] #['ssp126'] #['ssp370'] # #[\"historical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc33d931-94ca-4f84-895f-41fdc105fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2025.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2025.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2030.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2030.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2027.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2027.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2031.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2031.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2029.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2029.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2026.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2026.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2033.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2033.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2028.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2028.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2036.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2036.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2039.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2039.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2042.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2042.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2032.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2032.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2041.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2041.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2035.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2035.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2034.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2034.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2043.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2043.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2037.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2037.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2040.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2040.nc\n",
      "Downloading: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2038.nc\n",
      "Deleted original unfiltered file: pr_day_TaiESM1_ssp370_r1i1p1f1_gn_2038.nc\n"
     ]
    }
   ],
   "source": [
    "# Run the process\n",
    "filter_and_download('gddp-cmip6-thredds-fileserver.csv', years, models, variables, scenarios, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db81fb-7018-46dc-9602-8b89503fa60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a889d-f632-45ea-93d3-ae0722be0e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341d351-088c-4f37-859a-9027564baa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c436bc9-68a5-495f-844b-a033a2623614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f52cb2-d577-4173-9784-3d68c12456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_and_download_2(csv_path, years, models, variables, scenarios, north, south, east, west):\n",
    "#     with open(csv_path) as index:\n",
    "#         fobjects = csv.reader(index)\n",
    "#         next(fobjects)  # Skip header\n",
    "\n",
    "#         # Filter and download each file that matches the criteria\n",
    "#         for objs in fobjects:\n",
    "#             md5, uri = [o.strip() for o in objs]\n",
    "\n",
    "#             # Extract the filename from the URL path\n",
    "#             prsout = urlparse(uri)\n",
    "#             ofile = os.path.split(prsout.path)[1]\n",
    "\n",
    "#             # Check if the file matches filtering criteria\n",
    "#             if any(str(year) in ofile for year in years) and \\\n",
    "#                any(model in ofile for model in models) and \\\n",
    "#                any(variable in ofile for variable in variables) and \\\n",
    "#                any(scenario in ofile for scenario in scenarios):\n",
    "                \n",
    "#                 print(f\"Downloading subset for: {ofile}\")  # Debug print\n",
    "                \n",
    "#                 # Construct the NCSS request URL\n",
    "#                 base_url = uri.replace(\"fileServer\", \"ncss/grid\")\n",
    "#                 subset_url = (\n",
    "#                     f\"{base_url}?var={variables[0]}&north={north}&south={south}\"\n",
    "#                     f\"&east={east}&west={west}&horizStride=1\"\n",
    "#                     f\"&time_start={years[0]}-01-01T00:00:00Z\"\n",
    "#                     f\"&time_end={years[-1]}-12-31T23:59:59Z\"\n",
    "#                     f\"&accept=netcdf3&addLatLon=true\"\n",
    "#                 )\n",
    "\n",
    "#                 # Define output filename\n",
    "#                 filename = os.path.splitext(os.path.basename(uri))[0] + \"_subset.nc\"\n",
    "\n",
    "#                 # Use wget to download the subset\n",
    "#                 subprocess.run([\"wget\", \"-O\", filename, subset_url])\n",
    "\n",
    "#                 print(f\"Downloaded: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4b4f3c-9a02-4476-aa92-f9e78b60f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# south=-36\n",
    "# north=-29\n",
    "# east=-60\n",
    "# west=-52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f3240f-a8e7-4231-bf5a-fee7c517041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the filtering parameters\n",
    "# years = '1985' #list(range(1985, 2015)) #list(range(2025, 2065)) #list(range(1985, 2015))\n",
    "# models=['TaiESM1'] #['EC-Earth3_'] #\n",
    "# variables = ['sfcWind'] #[\"pr\"]\n",
    "# scenarios = [\"historical\"] #['ssp370'] # #['ssp126']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0caca6e-ffaa-4665-abef-65c45f4ae93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_and_download_2('gddp-cmip6-thredds-fileserver.csv', years, models, variables, scenarios, north, south, east, west)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abdcf9-8316-4d67-b7a7-02ea805358d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
